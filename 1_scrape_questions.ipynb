{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0fwnbImmw60"
      },
      "outputs": [],
      "source": [
        "# Lightly edited web-scraping script from ChatGPT (o3 mini)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "\n",
        "def fetch_category_members(category, limit=500):\n",
        "    \"\"\"\n",
        "    Fetches all page titles in a given MediaWiki category.\n",
        "    \"\"\"\n",
        "    url = 'https://millionaire.fandom.com/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'list': 'categorymembers',\n",
        "        'cmtitle': f'Category:{category}',\n",
        "        'cmlimit': limit,\n",
        "        'format': 'json'\n",
        "    }\n",
        "    titles = []\n",
        "    while True:\n",
        "        resp = requests.get(url, params=params).json()\n",
        "        members = resp['query']['categorymembers']\n",
        "        titles.extend([m['title'] for m in members])\n",
        "        if 'continue' in resp:\n",
        "            params.update(resp['continue'])\n",
        "        else:\n",
        "            break\n",
        "    return titles\n",
        "\n",
        "\n",
        "def fetch_page_html(title):\n",
        "    \"\"\"\n",
        "    Retrieves the rendered HTML content for a given page title.\n",
        "    \"\"\"\n",
        "    url = 'https://millionaire.fandom.com/api.php'\n",
        "    params = {\n",
        "        'action': 'parse',\n",
        "        'page': title,\n",
        "        'prop': 'text',\n",
        "        'format': 'json'\n",
        "    }\n",
        "    resp = requests.get(url, params=params).json()\n",
        "    return resp['parse']['text']['*']\n",
        "\n",
        "def parse_questions(html):\n",
        "    \"\"\"\n",
        "    Parses HTML content to extract question texts.\n",
        "    Supports both numbered lists and table-based formats.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    questions = []\n",
        "\n",
        "    for tr in soup.find_all('tr', style=re.compile('background-color:\\\\s*#000000', re.I)):\n",
        "      td = tr.find('td', colspan=\"2\")\n",
        "      if not td:\n",
        "        continue\n",
        "      b = td.find_all('b')\n",
        "      if len(b) == 1:\n",
        "        questions.append(b[0].get_text())\n",
        "\n",
        "    return questions\n",
        "\n",
        "\n",
        "def scrape_contestant_questions(category_name):\n",
        "    \"\"\"\n",
        "    Main function to scrape all contestants in the category and their questions.\n",
        "    \"\"\"\n",
        "    contestants = fetch_category_members(category_name)\n",
        "    all_data = {}\n",
        "    for title in contestants:\n",
        "        print(f\"Scraping: {title}\")\n",
        "        html = fetch_page_html(title)\n",
        "        questions = parse_questions(html)\n",
        "        all_data[title] = questions\n",
        "    return all_data\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    category = 'Contestants_from_the_U.S.'\n",
        "    data = scrape_contestant_questions(category)\n",
        "    # Save to JSON file\n",
        "    with open('millionaire_questions.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Scraped questions for {len(data)} contestants. Output saved to 'millionaire_questions.json'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the output here\n",
        "import json\n",
        "\n",
        "with open('millionaire_questions.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "all_questions = [x for qs in data.values() for x in qs]\n",
        "print('total questions:', len(all_questions))\n",
        "print('word count:', len(' '.join(all_questions).split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX2fZ7HOh6gL",
        "outputId": "6f20bb8b-5170-487d-87e3-1a92674e33bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total questions: 25865\n",
            "word count: 430258\n"
          ]
        }
      ]
    }
  ]
}